id: data_mlops_workflow
namespace: corn_mlops_flow

inputs:
  - id: model_performance_threshold
    type: FLOAT
    defaults: 0.7
  - id: target_stage
    type: STRING
    defaults: Production
  - id: force_deployment
    type: BOOLEAN
    defaults: false
  - id: model_selection
    type: SELECT
    displayName: "Select the model you are interested:"
    defaults: linear
    values:
      - linear
      - ridge
      - lasso
      - gbr

variables:
  mlflow_tracking_uri: "https://mlflow-server-453290981886.us-central1.run.app"
  gcp_project: "corn-yield-prediction-kenia"
  experiment_name: "corn-yield-predictors"
  model_name: "corn-yield-predictor"
  algorithm: "{{ inputs.model_selection}}"

tasks:
  - id: download_dataset
    type: io.kestra.plugin.scripts.python.Script
    description: Download corn farming dataset from Kaggle using authenticated API
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    env:
      KAGGLE_USERNAME: "{{ kv('KAGGLE_USERNAME') }}"
      KAGGLE_KEY: "{{ kv('KAGGLE_KEY') }}"
    beforeCommands:
    - pip install kagglehub
    - pip install kaggle
    outputFiles:
      - "corn_data.csv"
    script: |
      import kaggle
      kaggle.api.authenticate()
      handle = "japondo/corn-farming-data"
      print("Los archivos a descargar son: ", kaggle.api.dataset_list_files(handle).files)
      kaggle.api.dataset_download_files(handle, path=".", unzip=True)

  - id: clean_split
    type: io.kestra.plugin.scripts.python.Commands
    containerImage: ghcr.io/kestra-io/pydata:latest
    description: Clean the dataset and preparing for training stage
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    namespaceFiles:
      enabled: true
      include:
        - clean.py
    beforeCommands:
      - pip install pandas scikit-learn
    inputFiles:
      corn.csv: "{{ outputs.download_dataset.outputFiles['corn_data.csv'] }}"
    commands:
      - python clean.py
    outputFiles:
      - "data_splits/*.csv"
      - "vectorizer.pkl"

  - id: if_linear
    type: io.kestra.plugin.core.flow.If
    condition: "{{inputs.model_selection == 'linear'}}"
    then:
    - id: train_linear_model
      type: io.kestra.plugin.scripts.python.Commands
      containerImage: ghcr.io/kestra-io/pydata:latest
      description: Training the linear model
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      namespaceFiles:
        enabled: true
        include:
          - train_linear.py
      env:
        # MLflow configuration - using your Cloud Run URL
        MLFLOW_TRACKING_URI: "{{ vars.mlflow_tracking_uri }}"
        # Optional: Set experiment name
        MLFLOW_EXPERIMENT_NAME: "{{ vars.experiment_name }}"
        MODEL_NAME: "{{ vars.model_name}}"
        MODEL_TYPE: "{{ inputs.model_selection}}"
      beforeCommands:
        - pip install pandas numpy scikit-learn mlflow==3.1.4 google-cloud-storage
      inputFiles:
        X_train.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_train.csv'] }}"
        y_train.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_train.csv'] }}"
        X_val.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_val.csv'] }}"
        y_val.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_val.csv'] }}"
        dict_vectorizer: "{{ outputs.clean_split.outputFiles['vectorizer.pkl'] }}"
      commands:
        - python train_linear.py
      outputFiles:
        - "model_artifacts/**"
        - "*.pkl"
        - "*.json"

    - id: optimize_train_linear
      type: io.kestra.plugin.scripts.python.Commands
      containerImage: ghcr.io/kestra-io/pydata:latest
      description: Optimizer for linear regression model
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      namespaceFiles:
        enabled: true
        include:
          - optimize.py
      env:
        # MLflow configuration - using your Cloud Run URL
        MLFLOW_TRACKING_URI: "{{ vars.mlflow_tracking_uri }}"
        # Optional: Set experiment name
        MLFLOW_EXPERIMENT_NAME: "{{ vars.experiment_name }}"
        MODEL_NAME: "{{ vars.model_name}}"
        MODEL_TYPE: "{{ inputs.model_selection}}"
      beforeCommands:
        - pip install pandas numpy scikit-learn mlflow==3.1.4 google-cloud-storage hyperopt scipy
      inputFiles:
        X_train.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_train.csv'] }}"
        y_train.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_train.csv'] }}"
        X_val.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_val.csv'] }}"
        y_val.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_val.csv'] }}"
        X_test.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_val.csv'] }}"
        y_test.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_val.csv'] }}"
        dict_vectorizer: "{{ outputs.clean_split.outputFiles['vectorizer.pkl'] }}"
      commands:
        - python optimize.py
      outputFiles:
        - "model_artifacts/**"
        - "*.pkl"
        - "*.json"

    - id: compare_between_linear_and_optimized_models
      type: io.kestra.plugin.scripts.python.Commands
      containerImage: ghcr.io/kestra-io/pydata:latest
      description: Compare linear and optimized models to select the best one
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      namespaceFiles:
        enabled: true
        include:
          - compare_models.py
      inputFiles:
        raw_run_info.json: "{{ outputs.train_linear_model.outputFiles['model_artifacts/linear_run_info.json'] }}"
        optimized_run_info.json: "{{ outputs.optimize_train_linear.outputFiles['model_artifacts/optimized_run_info.json'] }}"
      commands:
        - python compare_models.py
      outputFiles:
        - "model_artifacts/**"
        - "*.pkl"
        - "*.json"

    - id: linear_registry_management
      type: io.kestra.plugin.scripts.python.Commands
      containerImage: ghcr.io/kestra-io/pydata:latest
      description: Manage model versions and transitions in MLflow registry
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      namespaceFiles:
        enabled: true
        include:
          - model_registry.py
      env:
        MLFLOW_TRACKING_URI: "{{ vars.mlflow_tracking_uri }}"
        MODEL_NAME: "{{ vars.model_name}}"
      beforeCommands:
        - pip install mlflow google-cloud-storage pandas
      inputFiles:
        run_info.json: "{{ outputs.compare_between_linear_and_optimized_models.outputFiles['model_artifacts/final_run_info.json'] }}"
      commands:
        - python model_registry.py
      outputFiles:
        - "*.json"

  - id: if_lasso
    type: io.kestra.plugin.core.flow.If
    condition: "{{inputs.model_selection == 'lasso'}}"
    then:
    - id: train_lasso_model
      type: io.kestra.plugin.scripts.python.Commands
      containerImage: ghcr.io/kestra-io/pydata:latest
      description: Training the ridge model
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      namespaceFiles:
        enabled: true
        include:
          - train_lasso.py
      env:
        # MLflow configuration - using your Cloud Run URL
        MLFLOW_TRACKING_URI: "{{ vars.mlflow_tracking_uri }}"
        # Optional: Set experiment name
        MLFLOW_EXPERIMENT_NAME: "{{ vars.experiment_name }}"
        MODEL_NAME: "{{ vars.model_name}}"
        MODEL_TYPE: "{{ inputs.model_selection}}"
      beforeCommands:
        - pip install pandas numpy scikit-learn mlflow==3.1.4 google-cloud-storage
      inputFiles:
        X_train.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_train.csv'] }}"
        y_train.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_train.csv'] }}"
        X_val.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_val.csv'] }}"
        y_val.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_val.csv'] }}"
        dict_vectorizer: "{{ outputs.clean_split.outputFiles['vectorizer.pkl'] }}"
      commands:
        - python train_lasso.py
      outputFiles:
        - "model_artifacts/**"
        - "*.pkl"
        - "*.json"

    - id: optimize_lasso_regressor
      type: io.kestra.plugin.scripts.python.Commands
      containerImage: ghcr.io/kestra-io/pydata:latest
      description: Optimizer for Ridge regressor model
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      namespaceFiles:
        enabled: true
        include:
          - optimize.py
      env:
        # MLflow configuration - using your Cloud Run URL
        MLFLOW_TRACKING_URI: "{{ vars.mlflow_tracking_uri }}"
        # Optional: Set experiment name
        MLFLOW_EXPERIMENT_NAME: "{{ vars.experiment_name }}"
        MODEL_NAME: "{{ vars.model_name}}"
        MODEL_TYPE: "{{ inputs.model_selection}}"
      beforeCommands:
        - pip install pandas numpy scikit-learn mlflow==3.1.4 google-cloud-storage hyperopt scipy
      inputFiles:
        X_train.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_train.csv'] }}"
        y_train.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_train.csv'] }}"
        X_val.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_val.csv'] }}"
        y_val.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_val.csv'] }}"
        X_test.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_val.csv'] }}"
        y_test.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_val.csv'] }}"
        dict_vectorizer: "{{ outputs.clean_split.outputFiles['vectorizer.pkl'] }}"
      commands:
        - python optimize.py
      outputFiles:
        - "model_artifacts/**"
        - "*.pkl"
        - "*.json"

    - id: compare_between_lasso_and_optimized_models
      type: io.kestra.plugin.scripts.python.Commands
      containerImage: ghcr.io/kestra-io/pydata:latest
      description: Compare linear and optimized models to select the best one
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      namespaceFiles:
        enabled: true
        include:
          - compare_models.py
      inputFiles:
        raw_run_info.json: "{{ outputs.train_lasso_model.outputFiles['model_artifacts/lasso_run_info.json'] }}"
        optimized_run_info.json: "{{ outputs.optimize_lasso_regressor.outputFiles['model_artifacts/optimized_run_info.json'] }}"
      commands:
        - python compare_models.py
      outputFiles:
        - "model_artifacts/**"
        - "*.pkl"
        - "*.json"

    - id: lasso_registry_management
      type: io.kestra.plugin.scripts.python.Commands
      containerImage: ghcr.io/kestra-io/pydata:latest
      description: Manage model versions and transitions in MLflow registry
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      namespaceFiles:
        enabled: true
        include:
          - model_registry.py
      env:
        MLFLOW_TRACKING_URI: "{{ vars.mlflow_tracking_uri }}"
        MODEL_NAME: "{{ vars.model_name}}"
      beforeCommands:
        - pip install mlflow google-cloud-storage pandas
      inputFiles:
        run_info.json: "{{ outputs.compare_between_lasso_and_optimized_models.outputFiles['model_artifacts/final_run_info.json'] }}"
      commands:
        - python model_registry.py
      outputFiles:
        - "*.json"

  - id: if_ridge
    type: io.kestra.plugin.core.flow.If
    condition: "{{inputs.model_selection == 'ridge'}}"
    then:
    - id: train_ridge_model
      type: io.kestra.plugin.scripts.python.Commands
      containerImage: ghcr.io/kestra-io/pydata:latest
      description: Training the ridge model
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      namespaceFiles:
        enabled: true
        include:
          - train_ridge.py
      env:
        # MLflow configuration - using your Cloud Run URL
        MLFLOW_TRACKING_URI: "{{ vars.mlflow_tracking_uri }}"
        # Optional: Set experiment name
        MLFLOW_EXPERIMENT_NAME: "{{ vars.experiment_name }}"
        MODEL_NAME: "{{ vars.model_name}}"
        MODEL_TYPE: "{{ inputs.model_selection}}"
      beforeCommands:
        - pip install pandas numpy scikit-learn mlflow==3.1.4 google-cloud-storage
      inputFiles:
        X_train.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_train.csv'] }}"
        y_train.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_train.csv'] }}"
        X_val.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_val.csv'] }}"
        y_val.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_val.csv'] }}"
        dict_vectorizer: "{{ outputs.clean_split.outputFiles['vectorizer.pkl'] }}"
      commands:
        - python train_ridge.py
      outputFiles:
        - "model_artifacts/**"
        - "*.pkl"
        - "*.json"

    - id: optimize_ridge_regressor
      type: io.kestra.plugin.scripts.python.Commands
      containerImage: ghcr.io/kestra-io/pydata:latest
      description: Optimizer for Ridge regressor model
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      namespaceFiles:
        enabled: true
        include:
          - optimize.py
      env:
        # MLflow configuration - using your Cloud Run URL
        MLFLOW_TRACKING_URI: "{{ vars.mlflow_tracking_uri }}"
        # Optional: Set experiment name
        MLFLOW_EXPERIMENT_NAME: "{{ vars.experiment_name }}"
        MODEL_NAME: "{{ vars.model_name}}"
        MODEL_TYPE: "{{ inputs.model_selection}}"
      beforeCommands:
        - pip install pandas numpy scikit-learn mlflow==3.1.4 google-cloud-storage hyperopt scipy
      inputFiles:
        X_train.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_train.csv'] }}"
        y_train.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_train.csv'] }}"
        X_val.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_val.csv'] }}"
        y_val.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_val.csv'] }}"
        X_test.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_val.csv'] }}"
        y_test.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_val.csv'] }}"
        dict_vectorizer: "{{ outputs.clean_split.outputFiles['vectorizer.pkl'] }}"
      commands:
        - python optimize.py
      outputFiles:
        - "model_artifacts/**"
        - "*.pkl"
        - "*.json"

    - id: compare_between_ridge_and_optimized_models
      type: io.kestra.plugin.scripts.python.Commands
      containerImage: ghcr.io/kestra-io/pydata:latest
      description: Compare linear and optimized models to select the best one
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      namespaceFiles:
        enabled: true
        include:
          - compare_models.py
      inputFiles:
        raw_run_info.json: "{{ outputs.train_ridge_model.outputFiles['model_artifacts/ridge_run_info.json'] }}"
        optimized_run_info.json: "{{ outputs.optimize_ridge_regressor.outputFiles['model_artifacts/optimized_run_info.json'] }}"
      commands:
        - python compare_models.py
      outputFiles:
        - "model_artifacts/**"
        - "*.pkl"
        - "*.json"

    - id: ridge_registry_management
      type: io.kestra.plugin.scripts.python.Commands
      containerImage: ghcr.io/kestra-io/pydata:latest
      description: Manage model versions and transitions in MLflow registry
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      namespaceFiles:
        enabled: true
        include:
          - model_registry.py
      env:
        MLFLOW_TRACKING_URI: "{{ vars.mlflow_tracking_uri }}"
        MODEL_NAME: "{{ vars.model_name}}"
      beforeCommands:
        - pip install mlflow google-cloud-storage pandas
      inputFiles:
        run_info.json: "{{ outputs.compare_between_ridge_and_optimized_models.outputFiles['model_artifacts/final_run_info.json'] }}"
      commands:
        - python model_registry.py
      outputFiles:
        - "*.json"

  - id: if_gradientboostingregressor
    type: io.kestra.plugin.core.flow.If
    condition: "{{inputs.model_selection == 'gradientboostingregressor'}}"
    then:
    - id: train_gbr_model
      type: io.kestra.plugin.scripts.python.Commands
      containerImage: ghcr.io/kestra-io/pydata:latest
      description: Training the ridge model
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      namespaceFiles:
        enabled: true
        include:
          - train_gbt.py
      env:
        # MLflow configuration - using your Cloud Run URL
        MLFLOW_TRACKING_URI: "{{ vars.mlflow_tracking_uri }}"
        # Optional: Set experiment name
        MLFLOW_EXPERIMENT_NAME: "{{ vars.experiment_name }}"
        MODEL_NAME: "{{ vars.model_name}}"
        MODEL_TYPE: "{{ inputs.model_selection}}"
      beforeCommands:
        - pip install pandas numpy scikit-learn mlflow==3.1.4 google-cloud-storage
      inputFiles:
        X_train.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_train.csv'] }}"
        y_train.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_train.csv'] }}"
        X_val.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_val.csv'] }}"
        y_val.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_val.csv'] }}"
        dict_vectorizer: "{{ outputs.clean_split.outputFiles['vectorizer.pkl'] }}"
      commands:
        - python train_gbt.py
      outputFiles:
        - "model_artifacts/**"
        - "*.pkl"
        - "*.json"

    - id: optimize_gb_regressor
      type: io.kestra.plugin.scripts.python.Commands
      containerImage: ghcr.io/kestra-io/pydata:latest
      description: Optimizer for Ridge regressor model
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      namespaceFiles:
        enabled: true
        include:
          - optimize.py
      env:
        # MLflow configuration - using your Cloud Run URL
        MLFLOW_TRACKING_URI: "{{ vars.mlflow_tracking_uri }}"
        # Optional: Set experiment name
        MLFLOW_EXPERIMENT_NAME: "{{ vars.experiment_name }}"
        MODEL_NAME: "{{ vars.model_name}}"
        MODEL_TYPE: "{{ inputs.model_selection}}"
      beforeCommands:
        - pip install pandas numpy scikit-learn mlflow==3.1.4 google-cloud-storage hyperopt scipy
      inputFiles:
        X_train.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_train.csv'] }}"
        y_train.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_train.csv'] }}"
        X_val.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_val.csv'] }}"
        y_val.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_val.csv'] }}"
        X_test.csv: "{{ outputs.clean_split.outputFiles['data_splits/X_val.csv'] }}"
        y_test.csv: "{{ outputs.clean_split.outputFiles['data_splits/y_val.csv'] }}"
        dict_vectorizer: "{{ outputs.clean_split.outputFiles['vectorizer.pkl'] }}"
      commands:
        - python optimize.py
      outputFiles:
        - "model_artifacts/**"
        - "*.pkl"
        - "*.json"

    - id: compare_between_gbr_and_optimized_models
      type: io.kestra.plugin.scripts.python.Commands
      containerImage: ghcr.io/kestra-io/pydata:latest
      description: Compare linear and optimized models to select the best one
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      namespaceFiles:
        enabled: true
        include:
          - compare_models.py
      inputFiles:
        raw_run_info.json: "{{ outputs.train_gbr_model.outputFiles['model_artifacts/gbt_run_info.json'] }}"
        optimized_run_info.json: "{{ outputs.optimize_gb_regressor.outputFiles['model_artifacts/optimized_run_info.json'] }}"
      commands:
        - python compare_models.py
      outputFiles:
        - "model_artifacts/**"
        - "*.pkl"
        - "*.json"

    - id: gbr_registry_management
      type: io.kestra.plugin.scripts.python.Commands
      containerImage: ghcr.io/kestra-io/pydata:latest
      description: Manage model versions and transitions in MLflow registry
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      namespaceFiles:
        enabled: true
        include:
          - model_registry.py
      env:
        MLFLOW_TRACKING_URI: "{{ vars.mlflow_tracking_uri }}"
        MODEL_NAME: "{{ vars.model_name}}"
      beforeCommands:
        - pip install mlflow google-cloud-storage pandas
      inputFiles:
        run_info.json: "{{ outputs.compare_between_gbr_and_optimized_models.outputFiles['model_artifacts/final_run_info.json'] }}"
      commands:
        - python model_registry.py
      outputFiles:
        - "*.json"

  - id: select_model_info
    type: io.kestra.plugin.core.flow.Switch
    value: "{{ inputs.model_selection }}"
    cases:
      linear:
        - id: get_linear_info
          type: io.kestra.plugin.scripts.python.Script
          inputFiles:
            source_run_info.json: "{{ outputs.linear_registry_management.outputFiles['run_info.json'] }}"
          script: |
            import shutil
            shutil.copy("source_run_info.json", "final_run_info.json")
            print("Linear model info selected")
          outputFiles:
            - final_run_info.json

      lasso:
        - id: get_lasso_info
          type: io.kestra.plugin.scripts.python.Script
          inputFiles:
            source_run_info.json: "{{ outputs.lasso_registry_management.outputFiles['run_info.json'] }}"
          script: |
            import shutil
            shutil.copy("source_run_info.json", "final_run_info.json")
            print("Lasso model info selected")
          outputFiles:
            - final_run_info.json

      ridge:
        - id: get_ridge_info
          type: io.kestra.plugin.scripts.python.Script
          inputFiles:
            source_run_info.json: "{{ outputs.ridge_registry_management.outputFiles['run_info.json'] }}"
          script: |
            import shutil
            shutil.copy("source_run_info.json", "final_run_info.json")
            print("Ridge model info selected")
          outputFiles:
            - final_run_info.json

      gbr:
        - id: get_gbr_info
          type: io.kestra.plugin.scripts.python.Script
          inputFiles:
            source_run_info.json: "{{ outputs.gbr_registry_management.outputFiles['run_info.json'] }}"
          script: |
            import shutil
            shutil.copy("source_run_info.json", "final_run_info.json")
            print("GBR model info selected")
          outputFiles:
            - final_run_info.json


  - id: fetch_production_model
    type: io.kestra.plugin.scripts.python.Commands
    containerImage: ghcr.io/kestra-io/pydata:latest
    description: Fetch the latest production model from MLflow and evaluate performance
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    namespaceFiles:
      enabled: true
      include:
        - model_fetch.py
    inputFiles:
      dict_vectorizer: "{{ outputs.clean_split.outputFiles['vectorizer.pkl'] }}"
      final_run_info.json: "{{ outputs.select_model_output.outputFiles['final_run_info.json'] }}"
    beforeCommands:
      - pip install mlflow pandas numpy scikit-learn
    env:
      MLFLOW_TRACKING_URI: "{{ vars.mlflow_tracking_uri}}"
      MODEL_NAME: "{{ vars.model_name}}"
      TARGET_STAGE: "{{ inputs.target_stage}}"
      THRESHOLD: "{{ inputs.model_performance_threshold }}"
      FORCE_DEPLOYMENT: "{{ inputs.force_deployment}}"
    commands:
      - python model_fetch.py
    outputFiles:
      - "*.bin"
      - "model_metadata.json"
      - "deployment_decision.json"

  - id: deployment_decision_gate
    type: io.kestra.plugin.core.flow.If
    condition: "{{ read(outputs.fetch_production_model.outputFiles['deployment_decision.json']) | jq('.should_deploy') }}"
    then:
      - id: build_docker_image
        type: io.kestra.plugin.scripts.shell.Commands
        description: Build Docker image for the model service
        inputFiles:
          model.bin: "{{ outputs.fetch_production_model.outputFiles['corn-yield-predictor_model.bin'] }}"
          model_metadata.json: "{{ outputs.fetch_production_model.outputFiles['model_metadata.json'] }}"
        commands:
        - |
          # Create app directory
          mkdir -p app

          # Move model files to app directory
          mv model.bin app/
          mv model_metadata.json app/

          # Create requirements.txt
          cat > app/requirements.txt << EOF
          flask==2.3.3
          pandas==2.3.1
          numpy==2.2.6
          scikit-learn==1.7.1
          waitress==2.0.0
          google-cloud-logging==3.8.0
          EOF

          # Create main.py (Flask app)
          cat > app/main.py << 'EOF'
          import os
          import json
          import logging
          import pickle
          from flask import Flask, request, jsonify
          import pandas as pd
          import numpy as np

          # The model used is referred
          model_file = 'model.bin'

          # Setup logging
          logging.basicConfig(level=logging.INFO)
          logger = logging.getLogger(__name__)

          # Load model and metadata at startup
          try:
              with open(model_file, 'rb') as f_in:
                  dv, model = pickle.load(f_in)

              with open('model_metadata.json', 'r') as f:
                  metadata = json.load(f)

              logger.info(f"Model loaded: {metadata['model_name']} v{metadata['model_version']}")
          except Exception as e:
              logger.error(f"Failed to load model: {e}")
              raise

          # Instantiating the app
          app = Flask('yield')

          @app.route('/predict', methods=['POST'])
          # Function that calculates the target variable:
          def predict():
              farmer = request.get_json()
              X = dv.transform([farmer])
              y_pred = model.predict(X)[0]
              result = {
                  'Yield prediction': y_pred,
              }
              return jsonify(result)

          if __name__ == "__main__":
              app.run(debug=True, host='0.0.0.0', port=9696)
          EOF

          # Create Dockerfile
          cat > Dockerfile << EOF
          FROM python:3.11-slim

          WORKDIR /app

          # Copy requirements and install dependencies
          COPY app/requirements.txt .
          RUN pip install --no-cache-dir -r requirements.txt

          # Copy application files
          COPY app/ .

          # Expose the application port
          EXPOSE 9696

          # Set the default command to run the application
          CMD ["waitress-serve", "--listen=0.0.0.0:9696", "main:app"]
          EOF

          # Archive the app folder
          tar -czf app.tar.gz app/

          echo "Docker files created successfully"

        outputFiles:
          - "Dockerfile"
          - "app/**"
          - "app.tar.gz"

      - id: build_and_push_to_gcr
        type: io.kestra.plugin.scripts.shell.Commands
        description: Build Docker image using cloudbuild.yaml
        docker:
          image: google/cloud-sdk:alpine
        inputFiles:
          Dockerfile: "{{ outputs.build_docker_image.outputFiles['Dockerfile'] }}"
          app_archive: "{{ outputs.build_docker_image.outputFiles['app.tar.gz'] }}"
          model_metadata.json: "{{ outputs.fetch_production_model.outputFiles['model_metadata.json'] }}"
          service_account_key.json: "{{ kv('GCP_SERVICE_ACCOUNT_KEY') }}"
        env:
          PROJECT_ID: "{{ kv('GCP_PROJECT_ID') }}"
          REGION: "{{ kv('GCP_REGION') }}"
          SERVICE_NAME: "corn-predictor-service"
        commands:
          - |
              set -e

              echo "=== Building with cloudbuild.yaml method ==="

              echo "1. Setup..."
              gcloud auth activate-service-account --key-file=service_account_key.json
              gcloud config set project $PROJECT_ID
              apk add --no-cache jq
              tar -xzf app_archive

              MODEL_VERSION=$(jq -r '.model_version // "1"' model_metadata.json)
              TIMESTAMP=$(date +%Y%m%d-%H%M%S)

              echo "Project: $PROJECT_ID"
              echo "Service: $SERVICE_NAME"
              echo "Version: $MODEL_VERSION"
              echo "Timestamp: $TIMESTAMP"

              echo "2. Creating cloudbuild.yaml..."
              cat > cloudbuild.yaml << EOF
              steps:
              # Build the Docker image
              - name: 'gcr.io/cloud-builders/docker'
                args:
                  - 'build'
                  - '-t'
                  - 'gcr.io/$PROJECT_ID/$SERVICE_NAME:latest'
                  - '.'

              # Tag with version
              - name: 'gcr.io/cloud-builders/docker'
                args:
                  - 'tag'
                  - 'gcr.io/$PROJECT_ID/$SERVICE_NAME:latest'
                  - 'gcr.io/$PROJECT_ID/$SERVICE_NAME:v$MODEL_VERSION'

              # Tag with timestamp
              - name: 'gcr.io/cloud-builders/docker'
                args:
                  - 'tag'
                  - 'gcr.io/$PROJECT_ID/$SERVICE_NAME:latest'
                  - 'gcr.io/$PROJECT_ID/$SERVICE_NAME:$TIMESTAMP'

              # Push all images
              images:
              - 'gcr.io/$PROJECT_ID/$SERVICE_NAME:latest'
              - 'gcr.io/$PROJECT_ID/$SERVICE_NAME:v$MODEL_VERSION'
              - 'gcr.io/$PROJECT_ID/$SERVICE_NAME:$TIMESTAMP'

              timeout: '1200s'

              options:
                # Disable log streaming to avoid permission issues
                logging: CLOUD_LOGGING_ONLY
              EOF

              echo "3. Starting build with config file..."
              BUILD_ID=$(gcloud builds submit \
                --config=cloudbuild.yaml \
                --project=$PROJECT_ID \
                --async \
                --format="value(name)" | sed 's|.*/||')

              echo "Build ID: $BUILD_ID"

              echo "4. Monitoring build..."
              while true; do
                  STATUS=$(gcloud builds describe $BUILD_ID --project=$PROJECT_ID --format="value(status)")
                  echo "Build status: $STATUS"

                  case $STATUS in
                      "SUCCESS")
                          echo "Build successful!"
                          break
                          ;;
                      "FAILURE"|"TIMEOUT"|"CANCELLED"|"INTERNAL_ERROR")
                          echo "Build failed: $STATUS"
                          exit 1
                          ;;
                      *)
                          sleep 20
                          ;;
                  esac
              done

              echo "5. Creating output..."
              cat > build_info.json << EOF
              {
                "build_status": "SUCCESS",
                "build_id": "$BUILD_ID",
                "project_id": "$PROJECT_ID",
                "service_name": "$SERVICE_NAME",
                "model_version": "$MODEL_VERSION",
                "timestamp": "$TIMESTAMP",
                "images": {
                  "latest": "gcr.io/$PROJECT_ID/$SERVICE_NAME:latest",
                  "version": "gcr.io/$PROJECT_ID/$SERVICE_NAME:v$MODEL_VERSION",
                  "timestamp": "gcr.io/$PROJECT_ID/$SERVICE_NAME:$TIMESTAMP"
                },
                "console_url": "https://console.cloud.google.com/cloud-build/builds/$BUILD_ID?project=$PROJECT_ID"
              }
              EOF

              echo "Process completed!"
              cat build_info.json

        outputFiles:
          - "build_info.json"
          - "cloudbuild.yaml"

    else:
    - id: skip_deployment
      type: io.kestra.plugin.core.log.Log
      message: "Deployment skipped - should_deploy is false"
