id: mlflow-model-evaluation
namespace: corn_mlops_flow

description: Extract production model from MLflow and evaluate with Evidently for data drift

inputs:
  - id: model_name
    type: STRING
    required: true
    description: "Name of the model in MLflow registry"
  - id: dataset_path
    type: STRING
    required: false
    defaults: "corn_add.csv"
    description: "Path to the new dataset for evaluation"

variables:
  mlflow_url: "https://mlflow-server-453290981886.us-central1.run.app/"
  evidently_ui_url: "https://evidently-ui-453290981886.us-central1.run.app/"
  evidently_url: "https://evidently-monitoring-c4tqdte5jq-uc.a.run.app"

tasks:
  - id: extract_production_model
    type: "io.kestra.plugin.scripts.python.Script"
    containerImage: ghcr.io/kestra-io/pydata:latest
    description: "Extract current production model from MLflow registry"
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    beforeCommands:
      - pip install mlflow==3.1.4 evidently==0.7.9 pandas scikit-learn 
    script: |
      import mlflow
      import mlflow.tracking
      from mlflow.artifacts import download_artifacts
      import json
      import os
      import joblib
      import shutil
      import pickle
      
      os.makedirs('mydir/artifacts', exist_ok=True)

      # Set MLflow tracking URI
      mlflow_tracking_uri = mlflow.set_tracking_uri("{{ vars.mlflow_url }}")
      model_name = "{{ inputs.model_name }}"
      
      # Get production model version
      client = mlflow.MlflowClient()
      
      try:
          # Get latest version in Production stage
          production_versions = client.get_latest_versions(
              model_name, 
              stages=["Production"]
          )
          
          if not production_versions:
              raise Exception(f"No model found in Production stage for {model_name}")
          
          latest_prod_version = production_versions[0]
          model_version = latest_prod_version.version          
          run_id = latest_prod_version.run_id

          print(f"MLflow URI: {mlflow_tracking_uri}")
          print(f"Run ID: {run_id}")
     
          print(f"Found production model: {model_name} version {model_version}")
          
          # Download the model
          print("Downloading model directory...")
          model_dir_path = f"runs:/{run_id}/model"
          local_model_dir = download_artifacts(model_dir_path)
              
          print(f"Downloaded model directory to: {local_model_dir}")
          print(f"Model directory contents: {os.listdir(local_model_dir)}")

          # Load MLflow model and save as simple pickle
          print("Loading and converting MLflow model...")
          model = mlflow.sklearn.load_model(model_dir_path)

          # Save the model
          model_output_path = 'mydir/artifacts/model.pkl'
          joblib.dump(model, model_output_path)
          print(f"Model saved to: {model_output_path}")

          # Copy vectorizer from downloaded model directory
          vectorizer_source = os.path.join(local_model_dir, 'vectorizer.pkl')
          vectorizer_output_path = 'mydir/artifacts/vectorizer.pkl'
          
          vectorizer_found = False
          if os.path.exists(vectorizer_source):
              shutil.copy2(vectorizer_source, vectorizer_output_path)
              vectorizer_found = True
              print(f"Vectorizer copied from {vectorizer_source} to {vectorizer_output_path}")
          else:
              print(f"Vectorizer not found at {vectorizer_source}")
              # List what's actually in the model directory
              print(f"Contents of {local_model_dir}: {os.listdir(local_model_dir)}")

          # Verify both files exist with sizes
          for artifact_name in ['model.pkl', 'vectorizer.pkl']:
              artifact_path = f'mydir/artifacts/{artifact_name}'
              if os.path.exists(artifact_path):
                  size = os.path.getsize(artifact_path)
                  print(f"{artifact_name}: {size} bytes")
              else:
                  if artifact_name == 'vectorizer.pkl':
                      print(f"Warning: {artifact_name} not found")
                  else:
                      raise FileNotFoundError(f"{artifact_name} missing after download")

          print("=== ARTIFACT DOWNLOAD COMPLETED SUCCESSFULLY ===")
          
          # Save model info for next tasks
          model_info = {
              "model_name": model_name,
              "model_version": model_version,
              "model_uri": model_dir_path,
              "model_stage": "Production",
              "vectorizer_found": vectorizer_found
          }
          
          with open("model_info.json", "w") as f:
              json.dump(model_info, f)

          # Show final directory contents
          print(f"Root directory contents: {os.listdir('.')}")
          print(f"Artifacts directory contents: {os.listdir('mydir/artifacts')}")

      except Exception as e:
          print(f"Error extracting model: {str(e)}")
          raise e
    outputFiles:
    - "model_info.json"
    - "mydir/artifacts/model.pkl"
    - "mydir/artifacts/vectorizer.pkl"

  - id: load_and_prepare_base_data
    type: "io.kestra.plugin.scripts.python.Script"
    containerImage: ghcr.io/kestra-io/pydata:latest
    description: "Extract data used for training the model in production"
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    env:
      KAGGLE_USERNAME: "{{ kv('KAGGLE_USERNAME') }}"
      KAGGLE_KEY: "{{ kv('KAGGLE_KEY') }}"
    beforeCommands:
    - pip install kagglehub
    - pip install kaggle 
    - pip install mlflow==3.1.4 evidently==0.7.9 pandas scikit-learn numpy
    outputFiles:
    - "X_encoded.npy"
    - "y.npy" 
    - "feature_names.npy"
    script: |
      import os
      import kaggle
      import pandas as pd
      import numpy as np
      from sklearn.feature_extraction import DictVectorizer

      kaggle.api.authenticate()
      handle = "japondo/corn-farming-data"
      print("Los archivos a descargar son: ", kaggle.api.dataset_list_files(handle).files)
      kaggle.api.dataset_download_files(handle, path=".", unzip=True)

      # Find the CSV file in the current directory
      csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]
      if not csv_files:
          raise FileNotFoundError("No CSV files found after download")

      # Use the first CSV file found (or specify the exact filename if you know it)
      csv_filename = csv_files[0]
      print(f"Loading CSV file: {csv_filename}")

      # Load the dataset
      corn = pd.read_csv(csv_filename, sep=",")

      ## Step 1: Understanding the data
      print("The dataset size is: ", corn.shape)

      print("The columns in the dataset size are: ", corn.columns)

      # Using the info() method, we can quickly identify the data type of each column and detect null values:"
      corn.info()

      print("The number of null values in the dataset is confirmed as: ",corn.isna().sum())


      ## Step 2: Data preparation
      # Now that I have a general understanding of the data, some cleaning is needed before proceeding with further analysis.


      # Then, our subset selected for analysis is:
      corn_subset = corn[['Education', 'Gender', 'Age bracket',
                          'Household size', 'Acreage', 'Fertilizer amount', 'Laborers',
                          'Yield', 'Main credit source', 'Farm records', 
                          'Main advisory source', 'Extension provider', 'Advisory format', 
                          'Advisory language']]

      # Column names in our refined dataframe are converted to lowercase, and spaces are removed for consistency and usability:
      corn_subset.columns = [name.lower() for name in corn_subset.columns]
      corn_subset.columns = [name.replace(" ","_") for name in corn_subset.columns]


      # Those registries represent:
      missing_land = corn_subset['acreage'].isna().sum()
      amount_ml = (missing_land / corn.shape[0])*100
      print(f'The percentage of registries with missing values of cultivated land represent {amount_ml}')
      # While removing a large number of missing values is generally not advisable, the lack of access to the research team for clarification and the limited usefulness of this data for our model, these rows will be removed from the dataframe.

      # The resulting dataframe is:
      filter = corn_subset['acreage'].isna()
      corn_subset = corn_subset[~filter]

      # It makes sense that farmers in a developing country might have little to no formal education. Therefore, we can reasonably infer that many of them have not achieved any formal academic qualifications.
      # We populate the missing values in the education column with "No educated":
      corn_subset.loc[corn_subset['education'].isna()] = corn_subset.loc[corn_subset['education'].isna()].fillna('No educated')

      print("The main statistics for out clean dataset are: ",corn_subset.describe(include='all'))


      ## Step 3: Feature understanding
      
      # Now, it is important to understand how the selected variables behave:
      print("The target variable is Yield")
      print("There are no outliers visible at first glance")

      categorical_columns = ['education', 'age_bracket', 'household_size', 'laborers', 
                            'main_advisory_source', 'acreage']

      # The significant variables identifyed were:
      significant_var = ['education', 'age_bracket', 'household_size', 'laborers', 'main_advisory_source', 
                    'acreage', 'fertilizer_amount', 'yield']

      # The not significant variables identifyed were:
      not_significant_var = ['gender', 'main_credit_source', 'farm_records', 'extension_provider', 
                            'advisory_format', 'advisory_language']

      # The cleaned dataset is filtered to include the significant variables identified above.
      corn_cleaned = corn_subset[significant_var]
      corn.reset_index
      print("The final cleaned data is the following: ")
      print(corn_cleaned.head())


      # the Working dataset is prepared and splitted as follows:

      # Preparation dataset
      X = corn_cleaned.drop('yield', axis=1)
      y = corn_cleaned['yield']

      # Turning the dataframes into dictionaries:
      X_dic = X.to_dict(orient='records')


      # Instanciating the vectorizer for Hot Encoding:
      dv = DictVectorizer(sparse=False)

      # Applying the vectorizer:
      X_encoded = dv.fit_transform(X_dic)
  
      # Save X_encoded as numpy array
      np.save('X_encoded.npy', X_encoded)

      # Save y as numpy array  
      np.save('y.npy', y.values)

      # Also save feature names for reference
      feature_names = dv.get_feature_names_out()
      np.save('feature_names.npy', feature_names)
  
  - id: prepare_base_report
    type: io.kestra.plugin.scripts.python.Commands
    containerImage: ghcr.io/kestra-io/pydata:latest
    description: "Create the report for evaluation at current month (base performance)"
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    namespaceFiles:
      enabled: true
      include:
        - monitoring.py
    inputFiles:
      model_info.json: "{{ outputs.extract_production_model.outputFiles['model_info.json'] }}"
      model.pkl: "{{ outputs.extract_production_model.outputFiles['mydir/artifacts/model.pkl'] }}"
      vectorizer.pkl: "{{ outputs.extract_production_model.outputFiles['mydir/artifacts/vectorizer.pkl'] }}"
      X_encoded.npy: "{{ outputs.load_and_prepare_base_data.outputFiles['X_encoded.npy'] }}"
      y.npy: "{{ outputs.load_and_prepare_base_data.outputFiles['y.npy'] }}"
      feature_names.npy: "{{ outputs.load_and_prepare_base_data.outputFiles['feature_names.npy'] }}"
    beforeCommands:
      - pip install mlflow pandas==2.3.1 numpy==2.2.6 scikit-learn==1.7.1 mlflow==3.1.4 joblib==1.4.2
      - pip install evidently==0.7.9 requests
    commands:
      - python monitoring.py
    outputFiles:
    outputFiles:
    - project_metadata.json
    - "workspace/**"
    - "workspace.zip"

  - id: verify_cloud_upload
    type: "io.kestra.plugin.scripts.python.Script"
    containerImage: ghcr.io/kestra-io/pydata:latest
    description: "Verify upload to evidently cloud services and handle any retries"
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    inputFiles:
      project_metadata.json: "{{ outputs.prepare_base_report.outputFiles['project_metadata.json'] }}"
      workspace.zip: "{{ outputs.prepare_base_report.outputFiles['workspace.zip'] }}"
    env:
      evidently_ui_url: "{{ vars.evidently_ui_url}}"
      evidently_monitoring_url: "{{ vars.evidently_url}}"
    beforeCommands:
      - pip install requests pandas
    script: |
      import json
      import requests
      import os
      import zipfile

      # Load project metadata
      with open('project_metadata.json', 'r') as f:
          metadata = json.load(f)

      print("=== EVIDENTLY CLOUD UPLOAD VERIFICATION ===")
      print(f"Project ID: {metadata['project_id']}")
      print(f"Timestamp: {metadata['created_at']}")
      print(f"Report Name: {metadata['report_name']}")

      # Check upload status
      upload_status = metadata['upload_status']
      print(f"\nUpload Status:")
      print(f"  Monitoring Service: {'‚úÖ Success' if upload_status['monitoring_service'] else '‚ùå Failed'}")
      print(f"  UI Service: {'‚úÖ Success' if upload_status['ui_service'] else '‚ùå Failed'}")

      if 'error' in upload_status:
          print(f"  Error: {upload_status['error']}")

      # Loading the urls of evidently
      evidently_ui_url = os.getenv("evidently_ui_url")
      evidently_monitoring_url = os.getenv("evidently_monitoring_url")

      print(f"\n=== SERVICE HEALTH CHECK ===")

      try:
          ui_health = requests.get(f"{evidently_ui_url}/health", timeout=10)
          print(f"UI Service ({evidently_ui_url}): {'‚úÖ Available' if ui_health.status_code == 200 else '‚ùå Unavailable'}")
      except Exception as e:
          print(f"UI Service ({evidently_ui_url}): ‚ùå Error - {e}")

      try:
          monitoring_health = requests.get(f"{evidently_monitoring_url}/health", timeout=10)
          print(f"Monitoring Service ({evidently_monitoring_url}): {'‚úÖ Available' if monitoring_health.status_code == 200 else '‚ùå Unavailable'}")
      except Exception as e:
          print(f"Monitoring Service ({evidently_monitoring_url}): ‚ùå Error - {e}")

      print(f"\n=== AVAILABLE FILES ===")
      for root, dirs, files in os.walk('.'):
          for file in files:
              file_path = os.path.join(root, file)
              file_size = os.path.getsize(file_path)
              print(f"  {file_path}: {file_size} bytes")

      # Unzip workspace for inspection if needed
      if os.path.exists("workspace.zip"):
          with zipfile.ZipFile("workspace.zip", 'r') as zip_ref:
              zip_ref.extractall("workspace_extracted")
          print(f"\nWorkspace extracted to workspace_extracted/")

      # Create summary
      summary = {
          "verification_timestamp": metadata['timestamp'],
          "project_id": metadata['project_id'],
          "services_accessible": True,
          "files_ready": True,
          "next_steps": [
              "Monitor dashboards at: " + evidently_ui_url,
              "Access API at: " + evidently_monitoring_url,
              "Project files available in workspace.zip"
          ]
      }

      with open('verification_summary.json', 'w') as f:
          json.dump(summary, f, indent=2)

      print(f"\n=== VERIFICATION COMPLETE ===")
      print("Summary saved to verification_summary.json")
      print(f"üéâ Evidently monitoring setup complete!")
      print(f"üìä Dashboard: {evidently_ui_url}")
      print(f"üîß API: {evidently_monitoring_url}")



  - id: notify_completion
    type: "io.kestra.plugin.core.log.Log"
    message: |
      üéâ Evidently Monitoring Pipeline Complete!
      
      üìä Dashboard URL: https://evidently-ui-c4tqdte5jq-uc.a.run.app
      üîß Monitoring API: https://evidently-monitoring-c4tqdte5jq-uc.a.run.app
      
      Project ID: {{ outputs.verify_cloud_upload.vars.project_id }}
      
      All reports and dashboards have been uploaded to your Google Cloud Evidently services.
    level: INFO

  # - id: load_and_prepare_test_data
  #   type: "io.kestra.plugin.scripts.python.Script"
  #   containerImage: ghcr.io/kestra-io/pydata:latest
  #   description: "Load new dataset and prepare for evaluation"
  #   taskRunner:
  #     type: io.kestra.plugin.core.runner.Process
  #   beforeCommands:
  #     - pip install mlflow==3.1.4 evidently==0.7.9 pandas scikit-learn
  #   namespaceFiles:
  #     enabled: true
  #     include:
  #       - corn_add.csv 
  #   script: |
  #     import pandas as pd
  #     import pickle
  #     from sklearn.feature_extraction import DictVectorizer
  #     from sklearn.model_selection import train_test_split

  #     corn = pd.read_csv("corn_add.csv", sep="," )


  #     ## Step 1: Understanding the data
      
  #     print("The dataset size is: ", corn.shape)
  #     print("The columns in the dataset size are: ", corn.columns)

  #     corn.info()
  #     print("The number of null values in the dataset is confirmed as: ",corn.isna().sum())


  #     ## Step 2: Data preparation
  #     # Now that I have a general understanding of the data, some cleaning is needed before proceeding with further analysis.


  #     # Then, our subset selected for analysis is:
  #     corn_subset = corn[['Education', 'Gender', 'Age bracket',
  #                         'Household size', 'Acreage', 'Fertilizer amount', 'Laborers',
  #                         'Yield', 'Main credit source', 'Farm records', 
  #                         'Main advisory source', 'Extension provider', 'Advisory format', 
  #                         'Advisory language']]

  #     # Column names in our refined dataframe are converted to lowercase, and spaces are removed for consistency and usability:
  #     corn_subset.columns = [name.lower() for name in corn_subset.columns]
  #     corn_subset.columns = [name.replace(" ","_") for name in corn_subset.columns]


  #     # Those registries represent:
  #     missing_land = corn_subset['acreage'].isna().sum()
  #     amount_ml = (missing_land / corn.shape[0])*100
  #     print(f'The percentage of registries with missing values of cultivated land represent {amount_ml}')
  #     # While removing a large number of missing values is generally not advisable, the lack of access to the research team for clarification and the limited usefulness of this data for our model, these rows will be removed from the dataframe.

  #     # The resulting dataframe is:
  #     filter = corn_subset['acreage'].isna()
  #     corn_subset = corn_subset[~filter]

  #     # It makes sense that farmers in a developing country might have little to no formal education. Therefore, we can reasonably infer that many of them have not achieved any formal academic qualifications.
  #     # We populate the missing values in the education column with "No educated":
  #     corn_subset.loc[corn_subset['education'].isna()] = corn_subset.loc[corn_subset['education'].isna()].fillna('No educated')
  #     print("The main statistics for out clean dataset are: ",corn_subset.describe(include='all'))


  #     ## Step 3: Feature understanding
  #     # Now, it is important to understand how the selected variables behave:
  #     print("The target variable is Yield")
  #     print("There are no outliers visible at first glance")

  #     categorical_columns = ['education', 'age_bracket', 'household_size', 'laborers', 
  #                           'main_advisory_source', 'acreage']

  #     # The significant variables identifyed were:
  #     significant_var = ['education', 'age_bracket', 'household_size', 'laborers', 'main_advisory_source', 
  #                   'acreage', 'fertilizer_amount', 'yield']

  #     # The not significant variables identifyed were:
  #     not_significant_var = ['gender', 'main_credit_source', 'farm_records', 'extension_provider', 
  #                           'advisory_format', 'advisory_language']

  #     # The cleaned dataset is filtered to include the significant variables identified above.
  #     corn_cleaned = corn_subset[significant_var]
  #     corn.reset_index
  #     print("The final cleaned data is the following: ")
  #     print(corn_cleaned.head())


  #     # the Working dataset is prepared and splitted as follows:
  #     # Preparation dataset
  #     X = corn_cleaned.drop('yield', axis=1)
  #     y = corn_cleaned['yield']
  #     # Turning the dataframes into dictionaries:
  #     X_dic = X.to_dict(orient='records')
  #     # Instanciating the vectorizer for Hot Encoding:
  #     dv = DictVectorizer(sparse=False)
  #     # Applying the vectorizer:
  #     X_encoded = dv.fit_transform(X_dic)

  #     # More robust saving that handles both DataFrames and numpy arrays
  #     def save_to_csv(data, filename):
  #         if hasattr(data, 'to_csv'):  # It's already a DataFrame
  #             data.to_csv(filename, index=False)
  #         else:  # It's a numpy array or similar
  #             pd.DataFrame(data).to_csv(filename, index=False)

  #     # Save feature datasets
  #     save_to_csv(X_encoded, 'X_train.csv')

  #     # Save target datasets
  #     y.to_csv('target.csv', index=False)
    
  #   outputFiles:
  #     - "target.csv"
  #     - "X_encoded.csv"
  
  

  # # - id: prepare_following_report
  #   type: io.kestra.plugin.scripts.python.Commands
  #   containerImage: ghcr.io/kestra-io/pydata:latest
  #   description: "Create the report for evaluation at following month (next performance)"
  #   taskRunner:
  #     type: io.kestra.plugin.core.runner.Process
  #   namespaceFiles:
  #     enabled: true
  #     include:
  #       - monitoring.py
  #   inputFiles:
  #     model_info.json: "{{ outputs.extract_production_model.outputFiles['model_info.json'] }}"
  #     model.pkl: "{{ outputs.extract_production_model.outputFiles['mydir/artifacts/model.pkl'] }}"
  #     vectorizer.pkl: "{{ outputs.extract_production_model.outputFiles['mydir/artifacts/vectorizer.pkl'] }}"
  #     X_encoded_val.csv: "{{ outputs.load_and_prepare_data.outputFiles['X_encoded.csv']}}"
  #     target_val.csv: "{{ outputs.load_and_prepare_data.outputFiles['target.csv']}}"
      
  #     train_data_file: "{{ outputs.clean_split.outputFiles['data_splits/X_train.csv'] }}"
  #     train_labels_file: "{{ outputs.clean_split.outputFiles['data_splits/y_train.csv'] }}"
  #     val_data_file: "{{ outputs.clean_split.outputFiles['data_splits/X_val.csv'] }}"
  #     val_labels_file: "{{ outputs.clean_split.outputFiles['data_splits/y_val.csv'] }}"
  #   commands:
  #     - python monitoring.py
  #   outputFiles:
  #     - "/**"