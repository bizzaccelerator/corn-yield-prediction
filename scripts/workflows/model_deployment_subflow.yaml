id: model_deployment_subflow
namespace: corn_mlops_flow

description: Reusable deployment subflow for ML models

inputs:
  - id: model_performance_threshold
    type: FLOAT
    defaults: 0.7
  - id: target_stage
    type: STRING
    defaults: Production
  - id: force_deployment
    type: BOOLEAN
    defaults: false
  - id: vectorizer_file_path
    type: STRING
    description: Path to the vectorizer pickle file
  - id: final_run_info_file_path
    type: STRING
    description: Path to the final run info JSON file

variables:
  mlflow_tracking_uri: "https://mlflow-server-453290981886.us-central1.run.app"
  model_name: "corn-yield-predictor"

tasks:
  - id: fetch_and_compare_models
    type: io.kestra.plugin.scripts.python.Commands
    containerImage: ghcr.io/kestra-io/pydata:latest
    description: Fetch production model and compare with candidate model
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    namespaceFiles:
      enabled: true
      include:
        - model_fetch.py
    inputFiles:
      dict_vectorizer: "{{ inputs.vectorizer_file_path }}"
      final_run_info.json: "{{ inputs.final_run_info_file_path }}"
    beforeCommands:
      - pip install mlflow pandas numpy scikit-learn
    env:
      MLFLOW_TRACKING_URI: "{{ vars.mlflow_tracking_uri }}"
      MODEL_NAME: "{{ vars.model_name }}"
      CANDIDATE_RUN_INFO: "final_run_info.json"
      METRIC: "rmse"
      COMPARISON_THRESHOLD: "0.05"
      FORCE_DEPLOYMENT: "{{ inputs.force_deployment }}"
    commands:
      - python model_fetch.py
    outputFiles:
      - "deployment_decision.json"

  - id: deployment_decision_gate
    type: io.kestra.plugin.core.flow.If
    condition: "{{ read(outputs.fetch_and_compare_models.outputFiles[\"deployment_decision.json\"]) | jq(\".should_deploy\") | first }}"
    then:
      - id: register_model_in_mlflow
        type: io.kestra.plugin.scripts.python.Commands
        containerImage: ghcr.io/kestra-io/pydata:latest
        description: Register candidate model in MLflow and promote to Production
        taskRunner:
          type: io.kestra.plugin.core.runner.Process
        namespaceFiles:
          enabled: true
          include:
            - model_registry.py
        env:
          MLFLOW_TRACKING_URI: "{{ vars.mlflow_tracking_uri }}"
          MODEL_NAME: "{{ vars.model_name }}"
        beforeCommands:
          - pip install mlflow google-cloud-storage pandas
        inputFiles:
          run_info.json: "{{ inputs.final_run_info_file_path }}"
        commands:
          - python model_registry.py
        outputFiles:
          - "*.json"
      
      - id: extract_model_and_vectorizer
        type: io.kestra.plugin.scripts.python.Script
        containerImage: ghcr.io/kestra-io/pydata:latest
        description: Extracting model and vectorizer currently in production
        inputFiles:
          model_metadata.json: "{{ outputs.register_model_in_mlflow.outputFiles['model_metadata.json'] }}"
        beforeCommands:
          - pip3 install mlflow==3.1.4 google-cloud-storage==2.10.0 "protobuf>=3.20.0,<5.0.0"
        script: |
          import os
          import json
          import mlflow
          import mlflow.sklearn  
          from mlflow.artifacts import download_artifacts
          import joblib
          import shutil

          # Create app directory structure first
          os.makedirs('mydir/artifacts', exist_ok=True)

          print("=== MODEL METADATA VERIFICATION ===")
          print("Model metadata contents:")
          with open('model_metadata.json', 'r') as f:
              metadata = json.load(f)
              print(json.dumps(metadata, indent=2))

          print("=== STARTING ARTIFACT DOWNLOAD ===")

          # Setup MLflow
          mlflow.set_tracking_uri(metadata['mlflow_tracking_uri'])
          run_id = metadata['mlflow_run_id']
          
          print(f"MLflow URI: {metadata['mlflow_tracking_uri']}")
          print(f"Run ID: {run_id}")

          try:
              # Download the entire model directory
              print("Downloading model directory...")
              model_dir_path = f"runs:/{run_id}/model"
              local_model_dir = download_artifacts(model_dir_path)
              
              print(f"Downloaded model directory to: {local_model_dir}")
              print(f"Model directory contents: {os.listdir(local_model_dir)}")

              # Load MLflow model and save as simple pickle
              print("Loading and converting MLflow model...")
              model = mlflow.sklearn.load_model(model_dir_path)
              
              model_output_path = 'mydir/artifacts/model.pkl'
              joblib.dump(model, model_output_path)
              print(f"Model saved to: {model_output_path}")

              # Find and copy vectorizer
              vectorizer_source = os.path.join(local_model_dir, 'vectorizer.pkl')
              vectorizer_output_path = 'mydir/artifacts/vectorizer.pkl'
              
              if os.path.exists(vectorizer_source):
                  shutil.copy2(vectorizer_source, vectorizer_output_path)
                  print(f"Vectorizer copied to: {vectorizer_output_path}")
              else:
                  print(f"Vectorizer not found at expected location: {vectorizer_source}")
                  print("Searching for vectorizer in subdirectories...")
                  
                  found = False
                  for root, dirs, files in os.walk(local_model_dir):
                      if 'vectorizer.pkl' in files:
                          source = os.path.join(root, 'vectorizer.pkl')
                          shutil.copy2(source, vectorizer_output_path)
                          print(f"Found and copied vectorizer from: {source}")
                          found = True
                          break
                  
                  if not found:
                      raise FileNotFoundError(f"vectorizer.pkl not found in {local_model_dir}")

              # Verify both files exist with sizes
              for artifact_name in ['model.pkl', 'vectorizer.pkl']:
                  artifact_path = f'mydir/artifacts/{artifact_name}'
                  if os.path.exists(artifact_path):
                      size = os.path.getsize(artifact_path)
                      print(f"{artifact_name}: {size} bytes")
                  else:
                      raise FileNotFoundError(f"{artifact_name} missing after download")

              print("=== ARTIFACT DOWNLOAD COMPLETED SUCCESSFULLY ===")

          except Exception as e:
              print(f"DOWNLOAD FAILED: {e}")
              import traceback
              traceback.print_exc()
              raise e
        
        outputFiles:
          - "mydir/**"


      - id: build_docker_image
        type: io.kestra.plugin.scripts.shell.Commands
        description: Build Docker image for the model service
        inputFiles:
          model.pkl: "{{ outputs.extract_model_and_vectorizer.outputFiles['mydir/artifacts/model.pkl'] }}"
          vectorizer.pkl: "{{ outputs.extract_model_and_vectorizer.outputFiles['mydir/artifacts/vectorizer.pkl'] }}"
          model_metadata.json: "{{ outputs.register_model_in_mlflow.outputFiles['model_metadata.json'] }}"
        commands:
        - |
          # Create app directory
          mkdir -p app
          
          # Create requirements.txt
          cat > app/requirements.txt << EOF
          flask==2.3.3
          pandas==2.3.1
          numpy==2.2.6
          scikit-learn==1.7.1
          waitress==2.0.0
          google-cloud-logging==3.8.0
          mlflow==3.1.4
          joblib==1.4.2
          scipy==1.13.1
          EOF

          # Create main.py (Flask app)
          cat > app/main.py << 'EOF'
          import os
          import json
          import logging
          from flask import Flask, request, jsonify
          import pandas as pd
          import numpy as np
          import joblib
          from scipy import sparse

          # Setup logging
          logging.basicConfig(level=logging.INFO)
          logger = logging.getLogger(__name__)

          # Load model metadata
          with open('model_metadata.json', 'r') as f:
              metadata = json.load(f)

          # Load model and vectorizer from local files
          try:
              logger.info("Loading model and vectorizer from local files...")
              
              # Load the model
              model = joblib.load('model.pkl')
              logger.info("Model loaded successfully")
              
              # Load the vectorizer
              vectorizer = joblib.load('vectorizer.pkl')
              logger.info("Vectorizer loaded successfully")

          except Exception as e:
              logger.error(f"Failed to initialize model/vectorizer: {e}")
              raise

          # Instantiate the Flask app
          app = Flask('yield')

          def _transform_payload_with_vectorizer(payload: dict):
              """
              Apply DictVectorizer (or compatible transformer) to incoming JSON payload.
              Ensures dense ndarray for estimators that don't accept sparse input.
              """
              # Vectorizers typically expect a list of dicts
              X = vectorizer.transform([payload])

              # Some models (e.g., GBDT) expect dense input
              if sparse.issparse(X):
                  X = X.toarray()

              return X

          @app.route('/predict', methods=['POST'])
          def predict():
              try:
                  farmer_data = request.get_json()
                  if not isinstance(farmer_data, dict):
                      return jsonify({'error': 'Payload must be a single JSON object with feature:value pairs'}), 400

                  X = _transform_payload_with_vectorizer(farmer_data)
                  y_pred = model.predict(X)[0]

                  result = {
                      'yield_prediction': float(y_pred),
                      'model_version': metadata['model_version'],
                      'model_name': metadata['model_name']
                  }
                  return jsonify(result)
              except Exception as e:
                  logger.error(f"Prediction error: {e}")
                  return jsonify({'error': str(e)}), 500

          @app.route('/health', methods=['GET'])
          def health():
              return jsonify({
                  'status': 'healthy',
                  'model_name': metadata['model_name'],
                  'model_version': metadata['model_version']
              })

          if __name__ == "__main__":
              app.run(debug=True, host='0.0.0.0', port=9696)
          EOF

          # Copy files to app directory
          cp model_metadata.json app/
          cp model.pkl app/
          cp vectorizer.pkl app/

          # Create Dockerfile
          cat > Dockerfile << EOF
          FROM python:3.11-slim

          WORKDIR /app
          
          # Copy requirements and install dependencies
          COPY app/requirements.txt .
          RUN pip install --no-cache-dir -r requirements.txt
          
          # Copy application files
          COPY app/ .
          
          # Expose the application port
          EXPOSE 9696

          # Set the default command to run the application
          CMD ["waitress-serve", "--listen=0.0.0.0:9696", "main:app"]
          EOF
          
          # Archive the app folder
          tar -czf app.tar.gz app/

          echo "Docker files created successfully"
        
        outputFiles:
          - "Dockerfile"
          - "app/**"
          - "app.tar.gz"
        
      - id: build_and_push_to_gcr
        type: io.kestra.plugin.scripts.shell.Commands
        description: Build Docker image using cloudbuild.yaml
        docker:
          image: google/cloud-sdk:alpine
        inputFiles:
          Dockerfile: "{{ outputs.build_docker_image.outputFiles['Dockerfile'] }}"
          app_archive: "{{ outputs.build_docker_image.outputFiles['app.tar.gz'] }}"
          model_metadata.json: "{{ outputs.register_model_in_mlflow.outputFiles['model_metadata.json'] }}"
          service_account_key.json: "{{ kv('GCP_SERVICE_ACCOUNT_KEY') }}"
        env:
          PROJECT_ID: "{{ kv('GCP_PROJECT_ID') }}"
          REGION: "{{ kv('GCP_REGION') }}"
          SERVICE_NAME: "corn-predictor-service"
        commands:
          - |
              set -e
              
              echo "=== Building with cloudbuild.yaml method ==="
              
              echo "1. Setup..."
              gcloud auth activate-service-account --key-file=service_account_key.json
              gcloud config set project $PROJECT_ID
              apk add --no-cache jq
              tar -xzf app_archive
              
              MODEL_VERSION=$(jq -r '.model_version // "1"' model_metadata.json)
              TIMESTAMP=$(date +%Y%m%d-%H%M%S)
              
              echo "Project: $PROJECT_ID"
              echo "Service: $SERVICE_NAME"  
              echo "Version: $MODEL_VERSION"
              echo "Timestamp: $TIMESTAMP"
              
              echo "2. Creating cloudbuild.yaml..."
              cat > cloudbuild.yaml << EOF
              steps:
              # Build the Docker image
              - name: 'gcr.io/cloud-builders/docker'
                args: 
                  - 'build'
                  - '-t'
                  - 'gcr.io/$PROJECT_ID/$SERVICE_NAME:latest'
                  - '.'
              
              # Tag with version
              - name: 'gcr.io/cloud-builders/docker'
                args: 
                  - 'tag'
                  - 'gcr.io/$PROJECT_ID/$SERVICE_NAME:latest'
                  - 'gcr.io/$PROJECT_ID/$SERVICE_NAME:v$MODEL_VERSION'
              
              # Tag with timestamp  
              - name: 'gcr.io/cloud-builders/docker'
                args:
                  - 'tag' 
                  - 'gcr.io/$PROJECT_ID/$SERVICE_NAME:latest'
                  - 'gcr.io/$PROJECT_ID/$SERVICE_NAME:$TIMESTAMP'
              
              # Push all images
              images:
              - 'gcr.io/$PROJECT_ID/$SERVICE_NAME:latest'
              - 'gcr.io/$PROJECT_ID/$SERVICE_NAME:v$MODEL_VERSION' 
              - 'gcr.io/$PROJECT_ID/$SERVICE_NAME:$TIMESTAMP'
              
              timeout: '1200s'
              
              options:
                # Disable log streaming to avoid permission issues
                logging: CLOUD_LOGGING_ONLY
              EOF
              
              echo "3. Starting build with config file..."
              BUILD_ID=$(gcloud builds submit \
                --config=cloudbuild.yaml \
                --project=$PROJECT_ID \
                --async \
                --format="value(name)" | sed 's|.*/||')
              
              echo "Build ID: $BUILD_ID"
              
              echo "4. Monitoring build..."
              while true; do
                  STATUS=$(gcloud builds describe $BUILD_ID --project=$PROJECT_ID --format="value(status)")
                  echo "Build status: $STATUS"
                  
                  case $STATUS in
                      "SUCCESS")
                          echo "Build successful!"
                          break
                          ;;
                      "FAILURE"|"TIMEOUT"|"CANCELLED"|"INTERNAL_ERROR")
                          echo "Build failed: $STATUS"
                          exit 1
                          ;;
                      *)
                          sleep 20
                          ;;
                  esac
              done
              
              echo "5. Creating output..."
              cat > build_info.json << EOF
              {
                "build_status": "SUCCESS",
                "build_id": "$BUILD_ID", 
                "project_id": "$PROJECT_ID",
                "service_name": "$SERVICE_NAME",
                "model_version": "$MODEL_VERSION",
                "timestamp": "$TIMESTAMP",
                "images": {
                  "latest": "gcr.io/$PROJECT_ID/$SERVICE_NAME:latest",
                  "version": "gcr.io/$PROJECT_ID/$SERVICE_NAME:v$MODEL_VERSION",
                  "timestamp": "gcr.io/$PROJECT_ID/$SERVICE_NAME:$TIMESTAMP"
                },
                "console_url": "https://console.cloud.google.com/cloud-build/builds/$BUILD_ID?project=$PROJECT_ID"
              }
              EOF
              
              echo "Process completed!"
              cat build_info.json
              
        outputFiles:
          - "build_info.json"
          - "cloudbuild.yaml"
      
    else:
      - id: skip_deployment
        type: io.kestra.plugin.core.log.Log
        message: "Deployment skipped - candidate model not better than production or force_deployment is false"
      
      - id: skip_output
        type: io.kestra.plugin.core.debug.Return
        format: "Deployment skipped - candidate model not better than production or force_deployment is false"

  # The final information is
  - id: final_build_info
    type: io.kestra.plugin.core.debug.Return
    format: >-
      {{ outputs.containsKey('build_and_push_to_gcr') ? read(outputs.build_and_push_to_gcr.outputFiles ['build_info.json']) : null }}

  - id: final_skip_message
    type: io.kestra.plugin.core.debug.Return
    format: >-
      {{ outputs.containsKey('skip_output') ? outputs.skip_output.value : null  }}




outputs:
  - id: deployment_result
    type: JSON
    value: "{{ read(outputs.fetch_and_compare_models.outputFiles['deployment_decision.json']) }}"
  
  - id: build_info
    type: STRING
    value: "{{ outputs.final_build_info.value }}"

  - id: skip_message
    type: STRING
    value: "{{ outputs.final_skip_message.value }}"