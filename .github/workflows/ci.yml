name: Continuous Integration

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: '3.9'

jobs:
  lint-and-format:
    runs-on: ubuntu-latest
    name: Code Quality & Formatting

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install linting dependencies
      run: |
        pip install --upgrade pip
        pip install black isort flake8 mypy bandit safety

    - name: Check code formatting with Black
      run: |
        black --check --diff .
      continue-on-error: true

    - name: Check import sorting with isort
      run: |
        isort --check-only --diff .
      continue-on-error: true

    - name: Lint with flake8
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=88 --statistics
      continue-on-error: true

    - name: Type checking with mypy
      run: |
        mypy . --ignore-missing-imports --no-strict-optional
      continue-on-error: true

    - name: Security check with bandit
      run: |
        bandit -r . -f json -o bandit-report.json || true
        bandit -r . --severity-level medium
      continue-on-error: true

    - name: Check dependencies for security vulnerabilities
      run: |
        safety check
      continue-on-error: true

  test:
    runs-on: ubuntu-latest
    name: Run Tests
    needs: lint-and-format

    strategy:
      matrix:
        python-version: ['3.8', '3.9', '3.10']

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install pytest pytest-cov pytest-xdist hyperopt scikit-learn joblib

        if [ -f requirements.txt ]; then
            pip install -r requirements.txt
        fi
        if [ -f requirements-dev.txt ]; then
            pip install -r requirements-dev.txt
        fi
        if [ -f setup.py ]; then
            pip install -e .
        fi

    - name: Create mock data files for testing
      run: |
        cat > corn.csv << 'EOF'
        Education,Gender,Age bracket,Household size,Acreage,Fertilizer amount,Laborers,Yield,Main credit source,Farm records,Main advisory source,Extension provider,Advisory format,Advisory language
        Secondary,Male,26-35,5,2.5,100,3,1500,Bank,Yes,Extension,Government,Group,English
        Primary,Female,36-45,6,3.0,120,4,1600,Cooperative,No,Radio,NGO,Individual,Local
        Tertiary,Male,46-55,4,2.0,80,2,1400,Self,Yes,TV,Private,Digital,English
        EOF

        python << 'PYTHON_EOF'
import numpy as np
import pandas as pd
import pickle
import json
from sklearn.feature_extraction import DictVectorizer
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
import joblib
import os

# Create models directory
os.makedirs("models", exist_ok=True)

# Training/validation/test dimensions
n_train, n_val, n_test = 60, 20, 15
n_features = 10

# ===== ENCODED DATA (numpy arrays) =====
X_encoded_train = np.random.rand(n_train, n_features)
np.save("X_encoded.npy", X_encoded_train)
np.save("X_encoded_train.npy", X_encoded_train)

X_encoded_val = np.random.rand(n_val, n_features)
np.save("X_encoded_val.npy", X_encoded_val)

y_train_array = np.random.uniform(1000, 2000, n_train)
np.save("y.npy", y_train_array)

# ===== FEATURE NAMES =====
feature_names = np.array(['Education', 'Gender', 'Age_bracket', 'Household_size', 
                          'Acreage', 'Fertilizer_amount', 'Laborers', 'Main_credit_source',
                          'Farm_records', 'Main_advisory_source'])
np.save("feature_names.npy", feature_names)

# ===== CSV DATA (numeric only for sklearn) =====
X_train_df = pd.DataFrame({
    'Education': np.random.choice([0, 1, 2], n_train),
    'Gender': np.random.choice([0, 1], n_train),
    'Age_bracket': np.random.choice([0, 1, 2], n_train),
    'Household_size': np.random.randint(3, 8, n_train),
    'Acreage': np.random.uniform(1.5, 4.0, n_train),
    'Fertilizer_amount': np.random.randint(50, 150, n_train),
    'Laborers': np.random.randint(1, 6, n_train)
})
X_train_df.to_csv('X_train.csv', index=False)

y_train_df = pd.DataFrame({'Yield': y_train_array})
y_train_df.to_csv('y_train.csv', index=False)

X_val_df = pd.DataFrame({
    'Education': np.random.choice([0, 1, 2], n_val),
    'Gender': np.random.choice([0, 1], n_val),
    'Age_bracket': np.random.choice([0, 1, 2], n_val),
    'Household_size': np.random.randint(3, 8, n_val),
    'Acreage': np.random.uniform(1.5, 4.0, n_val),
    'Fertilizer_amount': np.random.randint(50, 150, n_val),
    'Laborers': np.random.randint(1, 6, n_val)
})
X_val_df.to_csv('X_val.csv', index=False)

y_val_df = pd.DataFrame({'Yield': np.random.uniform(1000, 2000, n_val)})
y_val_df.to_csv('y_val.csv', index=False)

X_test_df = pd.DataFrame({
    'Education': np.random.choice([0, 1, 2], n_test),
    'Gender': np.random.choice([0, 1], n_test),
    'Age_bracket': np.random.choice([0, 1, 2], n_test),
    'Household_size': np.random.randint(3, 8, n_test),
    'Acreage': np.random.uniform(1.5, 4.0, n_test),
    'Fertilizer_amount': np.random.randint(50, 150, n_test),
    'Laborers': np.random.randint(1, 6, n_test)
})
X_test_df.to_csv('X_test.csv', index=False)

y_test_df = pd.DataFrame({'Yield': np.random.uniform(1000, 2000, n_test)})
y_test_df.to_csv('y_test.csv', index=False)

# ===== DICT VECTORIZER =====
dv = DictVectorizer()
dummy_data = [{'feature_' + str(i): i for i in range(5)} for _ in range(10)]
dv.fit(dummy_data)
with open("dict_vectorizer", "wb") as f:
    pickle.dump(dv, f)

# ===== TRAINED MODELS =====
mock_model = GradientBoostingRegressor(n_estimators=10, random_state=42)
mock_model.fit(X_encoded_train, y_train_array)
joblib.dump(mock_model, "model.pkl")
joblib.dump(mock_model, "models/test_model.pkl")

prod_model = LinearRegression()
prod_model.fit(X_encoded_train, y_train_array)
joblib.dump(prod_model, "models/prod_model.pkl")

# ===== JSON CONFIGS =====
final_run_info = {
    "run_id": "test_run_123456",
    "model_name": "test_model",
    "model_version": 1,
    "metrics": {"rmse": 0.5, "r2": 0.8, "mae": 0.3, "mse": 0.25, "RMSE": 0.5, "R2": 0.8},
    "validation_metrics": {"rmse": 0.52, "r2": 0.78, "mae": 0.32, "mse": 0.27, "RMSE": 0.52, "R2": 0.78},
    "RMSE": 0.5,
    "R2": 0.8,
    "Val_RMSE": 0.52,
    "Val_R2": 0.78,
    "parameters": {"n_estimators": 100, "max_depth": 5},
    "model_path": "models/test_model.pkl"
}
with open("final_run_info.json", "w") as f:
    json.dump(final_run_info, f, indent=2)

model_info = {
    "run_id": "test_run_123456",
    "model_name": "GradientBoostingRegressor",
    "model_version": 1,
    "metrics": {"rmse": 0.5, "r2": 0.8, "mae": 0.3},
    "parameters": {"n_estimators": 100, "max_depth": 5, "learning_rate": 0.1},
    "feature_names": feature_names.tolist()
}
with open("model_info.json", "w") as f:
    json.dump(model_info, f, indent=2)

production_model_info = {
    "run_id": "prod_run_0001",
    "model_name": "test_model",
    "model_version": 0,
    "metrics": {"rmse": 0.6, "r2": 0.75, "RMSE": 0.6, "R2": 0.75},
    "parameters": {"n_estimators": 50, "max_depth": 3},
    "model_path": "models/prod_model.pkl"
}
with open("production_model_info.json", "w") as f:
    json.dump(production_model_info, f, indent=2)

print("✓ All mock data files created successfully")
PYTHON_EOF

    - name: Setup Kaggle credentials (mock)
      run: |
        mkdir -p ~/.config/kaggle
        echo '{"username":"test","key":"test"}' > ~/.config/kaggle/kaggle.json
        chmod 600 ~/.config/kaggle/kaggle.json

    - name: Run unit tests
      run: |
        export PYTHONPATH="${PYTHONPATH}:${PWD}:${PWD}/scripts"
        pytest tests/ -v --cov=. --cov-report=xml --cov-report=html --cov-report=term
      env:
        PYTHONPATH: ${{ github.workspace }}:${{ github.workspace }}/scripts

    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  build-and-test-docker:
    runs-on: ubuntu-latest
    name: Docker Build Test
    needs: [test]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Check if Dockerfile exists
      id: dockerfile-check
      run: |
        if [ -f Dockerfile ]; then
            echo "dockerfile_exists=true" >> $GITHUB_OUTPUT
        else
            echo "dockerfile_exists=false" >> $GITHUB_OUTPUT
        fi

    - name: Build Docker image
      if: steps.dockerfile-check.outputs.dockerfile_exists == 'true'
      run: |
        docker build -t corn-yield-prediction:test .

    - name: Test Docker container
      if: steps.dockerfile-check.outputs.dockerfile_exists == 'true'
      run: |
        docker run --rm corn-yield-prediction:test python -c "import sys; print('Python version:', sys.version)"

  dependency-check:
    runs-on: ubuntu-latest
    name: Dependency Analysis

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install pip-audit
      run: pip install pip-audit

    - name: Check for outdated dependencies
      run: |
        if [ -f requirements.txt ]; then
            pip install -r requirements.txt
            pip list --outdated
        fi
      continue-on-error: true

    - name: Audit dependencies for vulnerabilities
      run: |
        if [ -f requirements.txt ]; then
            pip-audit -r requirements.txt
        fi
      continue-on-error: true

  notebook-tests:
    runs-on: ubuntu-latest
    name: Notebook Validation

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install notebook dependencies
      run: |
        pip install jupyter nbconvert nbformat
        if [ -f requirements.txt ]; then
            pip install -r requirements.txt
        fi

    - name: Execute notebooks
      run: |
        find . -name "*.ipynb" -not -path "./.git/*" -not -path "./.*" | while read notebook; do
            echo "Testing notebook: $notebook"
            jupyter nbconvert --to notebook --execute --inplace "$notebook" || echo "❌ Failed: $notebook"
        done
      continue-on-error: true

  pre-commit-hooks:
    runs-on: ubuntu-latest
    name: Pre-commit Hooks

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install pre-commit
      run: pip install pre-commit

    - name: Run pre-commit on all files
      run: |
        if [ -f .pre-commit-config.yaml ]; then
            pre-commit run --all-files
        else
            echo "No .pre-commit-config.yaml found, skipping pre-commit checks"
        fi
      continue-on-error: true